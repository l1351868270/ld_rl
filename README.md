# æ¢¯åº¦çˆ†ç‚¸
[Logit Dynamics in Softmax Policy Gradient Methods](https://arxiv.org/pdf/2506.12912)

[SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning](https://arxiv.org/pdf/2509.02479)

# [ç†µå¡Œç¼©]

[The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/pdf/2505.22617)

[BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping](https://arxiv.org/pdf/2510.18927)

[Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/pdf/2512.05591)

[CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/pdf/2509.20712)

# è®­æ¨ä¸ä¸€è‡´

[Your Efficient RL Framework Secretly Brings You Off-Policy RL Training](https://fengyao.notion.site/off-policy-rl#245721e3f6c4802fa3ffce687eef2d0a)

[Small Leak Can Sink a Great Shipâ€”Boost RL Training on MoE with ğ‘°ğ’„ğ’†ğ‘·ğ’ğ’‘!](https://ringtech.notion.site/icepop)

[When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch](https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda)

[Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/pdf/2510.26788)

# å‚è€ƒä¹¦
[]
